{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "infinite-sudan",
   "metadata": {},
   "source": [
    "# Explainatory Notebook\n",
    "\n",
    "This notebook try to explain who works one of our minimal learning step.\n",
    "\n",
    "- There is no features here (momentum, tikhonov, etc...)\n",
    "- The class written below is not identical to ours, is just more minimal.\n",
    "- The last class in the notebook is minimal as the first but has lot of print statements, just watch the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "endangered-ensemble",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_dir = '../../'\n",
    "data_dir = project_dir + 'data/'\n",
    "\n",
    "import sys\n",
    "sys.path.append(project_dir + 'NN/')\n",
    "\n",
    "import numpy as np\n",
    "import importlib\n",
    "import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "functional-collins",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.activations import actv_funcs, dactv_funcs\n",
    "\n",
    "class Layer:\n",
    "    \"\"\"\n",
    "    Layer of the Neural Network.\n",
    "    \"\"\"\n",
    "    def __init__(self, unit_number, input_matrix, \n",
    "                 func = (\"sigmoid\",1), starting_points = 0.1,):\n",
    "        \n",
    "        self.input=np.array(input_matrix) # Storing the input matrix\n",
    "        self.unit_number=unit_number      # Storing the number of units\n",
    "        num_features = np.shape(self.input)[1] # Number of input features\n",
    "\n",
    "        self.weight=np.random.uniform(-starting_points,starting_points,\n",
    "                                      size=(unit_number, num_features ) )\n",
    "        self.bias=np.random.uniform(- starting_points, starting_points,\n",
    "                                    size = unit_number )\n",
    "\n",
    "        #Storing the activation function and his derivative\n",
    "        self.function, self.slope=func\n",
    "        self.func=lambda x : actv_funcs(self.function)(x,self.slope)\n",
    "        self.der_func=lambda x : dactv_funcs(self.function)(x,self.slope)\n",
    "\n",
    "    @property\n",
    "    def net(self):\n",
    "        \"\"\"\n",
    "        This property evaluate the dot product between the inputs and the\n",
    "        weight (adding the bias).\n",
    "        \"\"\"\n",
    "        return self.input.dot( self.weight.T ) + self.bias\n",
    "\n",
    "    @property\n",
    "    def out(self):\n",
    "        \"\"\"\n",
    "        This property return the output values of the net using the activation\n",
    "        function.\n",
    "        \"\"\"\n",
    "        return self.func(self.net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "english-garden",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Implementation of the Multi Layer Perceptron.\n",
    "\"\"\"\n",
    "class MLP:\n",
    "    \"\"\"\n",
    "    Multi Layer Perceptron class.\n",
    "    \"\"\"\n",
    "    def __init__(self, structure = [], func = None, starting_points=None,\n",
    "                 filename = None, epoch_to_restore = -1):\n",
    "        self.network=[]\n",
    "        self.structure=structure # Number of units per layer\n",
    "        self.func=[f if isinstance(f, (tuple, list)) else (f, 1) for f in func]\n",
    "        if starting_points == None: self.starting_points = [0.1]*len(self.structure)\n",
    "        else: self.starting_points=starting_points # start_point list for random weights\n",
    "        self.epoch = 0 # set number of epoch to 0\n",
    "\n",
    "    def __getattr__(self,attr):\n",
    "        \"\"\"Get the atribute of MLP\"\"\"\n",
    "        return [getattr(lay,attr) for lay in self.network]\n",
    "\n",
    "    def train(self, input_data, labels, epoch, eta=0.1):\n",
    "\n",
    "        self.eta = eta # Learning rate\n",
    "\n",
    "        self.create_net(input_data)\n",
    "\n",
    "        # Start train the net\n",
    "        for i in range(epoch):\n",
    "            self.feedforward()\n",
    "            self.learning_step(labels)\n",
    "            self.epoch += 1\n",
    "\n",
    "\n",
    "    def predict(self, data):\n",
    "        self.network[0].input = data\n",
    "        self.feedforward()\n",
    "        return self.network[-1].out\n",
    "\n",
    "    def create_net(self, input_data):\n",
    "        for layer,num_unit in enumerate(self.structure):\n",
    "            if layer==0:\n",
    "                self.network.append(Layer(num_unit,input_data,\n",
    "                                    starting_points = self.starting_points[layer],\n",
    "                                    func=self.func[layer]))\n",
    "            else:\n",
    "                self.network.append(Layer(num_unit,self.network[layer-1].out,\n",
    "                                    starting_points = self.starting_points[layer],\n",
    "                                    func=self.func[layer]))\n",
    "                \n",
    "    def feedforward(self):\n",
    "        for lay_prev,lay_next in zip(self.network[:-1:],self.network[1::]):\n",
    "            lay_next.input=lay_prev.out\n",
    "\n",
    "    def learning_step(self,labels):\n",
    "        for reverse_layer_number,layer in enumerate(self.network[::-1]):\n",
    "            if reverse_layer_number==0:\n",
    "                delta=((labels-layer.out)*layer.der_func(layer.net))\n",
    "            else:\n",
    "                delta=(np.matmul(delta,weight_1)*layer.der_func(layer.net))\n",
    "            weight_1=layer.weight\n",
    "\n",
    "            grad_W=np.sum([np.outer(i,j) for i,j in zip(delta,layer.input)], axis=0) #batch\n",
    "            grad_b=np.sum(delta,axis=0)\n",
    "\n",
    "            layer.weight+=self.eta*grad_W \n",
    "            layer.bias  +=self.eta*grad_b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "herbal-novelty",
   "metadata": {},
   "source": [
    "## Define the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "round-charlotte",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = [[0,0],\n",
    "              [0,1],\n",
    "              [1,0],\n",
    "              [1,1]]\n",
    "input_data = np.array(input_data)\n",
    "labels = np.array([1,0,0,1]).reshape((len(input_data), 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "productive-today",
   "metadata": {},
   "source": [
    "# Lets train the model and evaluate the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "south-external",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.98128081]\n",
      " [0.02129464]\n",
      " [0.01794481]\n",
      " [0.98310307]]\n",
      "\n",
      "XOR learned.\n"
     ]
    }
   ],
   "source": [
    "structure=[2,np.shape(labels)[1]]\n",
    "act_func=[\"sigmoid\",\"sigmoid\"]\n",
    "start=[2,]*2\n",
    "\n",
    "netw = MLP(structure, func=act_func,\n",
    "           starting_points=start)\n",
    "\n",
    "N_epoch = 2000\n",
    "learning_rate=2\n",
    "netw.train(input_data, labels,\n",
    "           epoch = N_epoch, eta = learning_rate)\n",
    "print(netw.predict(input_data))\n",
    "\n",
    "\n",
    "if (netw.predict(input_data) - labels < 0.5).all():\n",
    "    print('\\nXOR learned.')\n",
    "else: print('\\nXOR not learned, local minima reached... Please try again.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hidden-pattern",
   "metadata": {},
   "source": [
    "## Go in deep of one epoch, print each learning step!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "numerical-biography",
   "metadata": {},
   "source": [
    "Define a verbose class that print what really happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "reasonable-residence",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Implementation of the Multi Layer Perceptron.\n",
    "\"\"\"\n",
    "class verbose_MLP:\n",
    "    \"\"\"\n",
    "    Multi Layer Perceptron class.\n",
    "    \"\"\"\n",
    "    def __init__(self, structure = [], func = None, starting_points=None,\n",
    "                 filename = None, epoch_to_restore = -1):\n",
    "        self.network=[]\n",
    "        self.structure=structure # Number of units per layer\n",
    "        self.func=[f if isinstance(f, (tuple, list)) else (f, 1) for f in func]\n",
    "        if starting_points == None: self.starting_points = [0.1]*len(self.structure)\n",
    "        else: self.starting_points=starting_points # start_point list for random weights\n",
    "        self.epoch = 0 # set number of epoch to 0\n",
    "\n",
    "    def __getattr__(self,attr):\n",
    "        \"\"\"Get the atribute of MLP\"\"\"\n",
    "        return [getattr(lay,attr) for lay in self.network]\n",
    "\n",
    "    def train(self, input_data, labels, epoch, eta=0.1):\n",
    "\n",
    "        self.eta = eta # Learning rate\n",
    "\n",
    "        self.create_net(input_data)\n",
    "\n",
    "        # Start train the net\n",
    "        for i in range(epoch):\n",
    "            print(f\"Epoch {i}\")\n",
    "            self.feedforward()\n",
    "            self.learning_step(labels)\n",
    "            self.epoch += 1\n",
    "\n",
    "\n",
    "    def predict(self, data):\n",
    "        self.network[0].input = data\n",
    "        self.feedforward()\n",
    "        return self.network[-1].out\n",
    "\n",
    "    def create_net(self, input_data):\n",
    "        for layer,num_unit in enumerate(self.structure):\n",
    "            if layer==0:\n",
    "                self.network.append(Layer(num_unit,input_data,\n",
    "                                    starting_points = self.starting_points[layer],\n",
    "                                    func=self.func[layer]))\n",
    "            else:\n",
    "                self.network.append(Layer(num_unit,self.network[layer-1].out,\n",
    "                                    starting_points = self.starting_points[layer],\n",
    "                                    func=self.func[layer]))\n",
    "                \n",
    "    def feedforward(self):\n",
    "        print(\"\\nFILLING THE NETWORK:\")\n",
    "        i = 1\n",
    "        for lay_prev,lay_next in zip(self.network[:-1:],self.network[1::]):\n",
    "            print(f\"\\nLayer {i-1}\")\n",
    "            print(f\" - {lay_prev.weight.shape[0]} Neuron\")\n",
    "            print(f\" - {lay_prev.weight.shape[1]} Input Features\")\n",
    "            print(f\"\\n -----> Input in layer {i-1}:\")\n",
    "            print('\\t' + str(lay_prev.input).replace('\\n', '\\n\\t'))\n",
    "            print(f\"\\n -----> Weights in layer {i-1}:\")\n",
    "            print('\\t' + str(lay_prev.weight).replace('\\n', '\\n\\t'))\n",
    "            print(f\"\\n -----> Bias in layer {i-1}:\")\n",
    "            print('\\t' + str(lay_prev.bias).replace('\\n', '\\n\\t'))\n",
    "            print(f\"\\n -----> input * W.T + bias in layer {i-1}:\")\n",
    "            print('\\t' + str(lay_prev.net).replace('\\n', '\\n\\t'))\n",
    "            print(f\"\\n -----> Output (actv_f(net)) from layer {i-1}:\")\n",
    "            print('\\t' + str(lay_prev.out).replace('\\n', '\\n\\t'))\n",
    "            \n",
    "            print(f\"\\nFill the layer {i} input with output from layer {i-1}.\")\n",
    "\n",
    "            lay_next.input=lay_prev.out\n",
    "            i +=1\n",
    "        \n",
    "        print(f\"\\nLayer {i-1}\")\n",
    "        print(f\" - {lay_next.weight.shape[0]} Neuron\")\n",
    "        print(f\" - {lay_next.weight.shape[1]} Input Features\")\n",
    "        print(f\"\\n -----> Input in layer {i-1}:\")\n",
    "        print('\\t' + str(lay_next.input).replace('\\n', '\\n\\t'))\n",
    "        print(f\"\\n -----> Weights in layer {i-1}:\")\n",
    "        print('\\t' + str(lay_next.weight).replace('\\n', '\\n\\t'))\n",
    "        print(f\"\\n -----> Bias in layer {i-1}:\")\n",
    "        print('\\t' + str(lay_next.bias).replace('\\n', '\\n\\t'))\n",
    "        print(f\"\\n -----> input * W.T + bias in layer {i-1}:\")\n",
    "        print('\\t' + str(lay_next.net).replace('\\n', '\\n\\t'))\n",
    "        print(f\"\\n -----> Output (actv_f(net)) from layer {i-1} = output of the net:\")\n",
    "        print('\\t' + str(lay_next.out).replace('\\n', '\\n\\t'))\n",
    "        \n",
    "\n",
    "\n",
    "    def learning_step(self,labels):\n",
    "        print(\"\\nLEARNING (BACKPROPAGATION)\")\n",
    "        i = len(self.network) - 1\n",
    "        for reverse_layer_number,layer in enumerate(self.network[::-1]):\n",
    "            print(f\"\\nLayer {i}\")\n",
    "            if reverse_layer_number==0:\n",
    "                print(f\"\\n -----> labels-out of layer {i}:\")\n",
    "                print('\\t' + str((labels-layer.out)).replace('\\n', '\\n\\t'))\n",
    "                delta=((labels-layer.out)*layer.der_func(layer.net))\n",
    "                print(f\"\\n -----> delta of layer {i}:\")\n",
    "                print('\\t' + str(delta).replace('\\n', '\\n\\t'))\n",
    "            else:\n",
    "                delta=(np.matmul(delta,weight_1)*layer.der_func(layer.net))\n",
    "                print(f\"\\n -----> delta of layer {i}:\")\n",
    "                print('\\t' + str(delta).replace('\\n', '\\n\\t'))\n",
    "            weight_1=layer.weight\n",
    "            print(\"\\n -----> (Rounded results)\")\n",
    "            print(f\"\\n -----> delta x inputs of layer {i} (tensor prod.):\")\n",
    "            tens_prod = np.round(np.array([np.outer(i,j) for i,j in zip(delta,layer.input)]), decimals = 3)\n",
    "            print('\\t' + str(tens_prod).replace('\\n', '\\n\\t'))\n",
    "            \n",
    "            grad_W=np.sum([np.outer(i,j) for i,j in zip(delta,layer.input)], axis=0) #batch\n",
    "            \n",
    "            print(f\"\\n -----> grad = sum(delta x inputs) of layer {i}:\")\n",
    "            print('\\t' + str(np.round(grad_W, decimals = 3)).replace('\\n', '\\n\\t'))\n",
    "\n",
    "            grad_b=np.sum(delta,axis=0)\n",
    "\n",
    "            layer.weight+=self.eta*grad_W \n",
    "            layer.bias  +=self.eta*grad_b\n",
    "            i = i-1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lyric-indonesian",
   "metadata": {},
   "source": [
    "## Define a new model with 3 hidden neurons and just one output for the task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "super-atlanta",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's train the model with [epoch: 1] and [eta: 1]\n",
      "\n",
      "Epoch 0\n",
      "\n",
      "FILLING THE NETWORK:\n",
      "\n",
      "Layer 0\n",
      " - 3 Neuron\n",
      " - 2 Input Features\n",
      "\n",
      " -----> Input in layer 0:\n",
      "\t[[0 0]\n",
      "\t [0 1]\n",
      "\t [1 0]\n",
      "\t [1 1]]\n",
      "\n",
      " -----> Weights in layer 0:\n",
      "\t[[-1.15603991 -1.09376286]\n",
      "\t [-0.03910188  1.84727646]\n",
      "\t [-0.16393459 -1.18237623]]\n",
      "\n",
      " -----> Bias in layer 0:\n",
      "\t[-1.71924976 -1.89543105  1.27512321]\n",
      "\n",
      " -----> input * W.T + bias in layer 0:\n",
      "\t[[-1.71924976 -1.89543105  1.27512321]\n",
      "\t [-2.81301262 -0.04815458  0.09274698]\n",
      "\t [-2.87528968 -1.93453293  1.11118862]\n",
      "\t [-3.96905254 -0.08725646 -0.07118761]]\n",
      "\n",
      " -----> Output (actv_f(net)) from layer 0:\n",
      "\t[[0.15196782 0.13062646 0.7816185 ]\n",
      "\t [0.05662504 0.48796368 0.52317014]\n",
      "\t [0.05338869 0.1262497  0.75235064]\n",
      "\t [0.01854106 0.47819971 0.48221061]]\n",
      "\n",
      "Fill the layer 1 input with output from layer 0.\n",
      "\n",
      "Layer 1\n",
      " - 1 Neuron\n",
      " - 3 Input Features\n",
      "\n",
      " -----> Input in layer 1:\n",
      "\t[[0.15196782 0.13062646 0.7816185 ]\n",
      "\t [0.05662504 0.48796368 0.52317014]\n",
      "\t [0.05338869 0.1262497  0.75235064]\n",
      "\t [0.01854106 0.47819971 0.48221061]]\n",
      "\n",
      " -----> Weights in layer 1:\n",
      "\t[[-1.36671404  0.69245935 -0.47734453]]\n",
      "\n",
      " -----> Bias in layer 1:\n",
      "\t[0.80218167]\n",
      "\n",
      " -----> input * W.T + bias in layer 1:\n",
      "\t[[0.31183731]\n",
      "\t [0.81295404]\n",
      "\t [0.45750692]\n",
      "\t [0.87779461]]\n",
      "\n",
      " -----> Output (actv_f(net)) from layer 1 = output of the net:\n",
      "\t[[0.57733366]\n",
      "\t [0.69273864]\n",
      "\t [0.61242258]\n",
      "\t [0.706365  ]]\n",
      "\n",
      "LEARNING (BACKPROPAGATION)\n",
      "\n",
      "Layer 1\n",
      "\n",
      " -----> labels-out of layer 1:\n",
      "\t[[ 0.42266634]\n",
      "\t [-0.69273864]\n",
      "\t [-0.61242258]\n",
      "\t [ 0.293635  ]]\n",
      "\n",
      " -----> delta of layer 1:\n",
      "\t[[ 0.10313883]\n",
      "\t [-0.14745068]\n",
      "\t [-0.14536534]\n",
      "\t [ 0.06090386]]\n",
      "\n",
      " -----> (Rounded results)\n",
      "\n",
      " -----> delta x inputs of layer 1 (tensor prod.):\n",
      "\t[[[ 0.016  0.013  0.081]]\n",
      "\t\n",
      "\t [[-0.008 -0.072 -0.077]]\n",
      "\t\n",
      "\t [[-0.008 -0.018 -0.109]]\n",
      "\t\n",
      "\t [[ 0.001  0.029  0.029]]]\n",
      "\n",
      " -----> grad = sum(delta x inputs) of layer 1:\n",
      "\t[[ 0.001 -0.048 -0.077]]\n",
      "\n",
      "Layer 0\n",
      "\n",
      " -----> delta of layer 0:\n",
      "\t[[-0.01815698  0.00755185 -0.00975078]\n",
      "\t [ 0.01075962 -0.02375356  0.02037322]\n",
      "\t [ 0.01003551 -0.01033885  0.01500116]\n",
      "\t [-0.00151394  0.00979833 -0.0084225 ]]\n",
      "\n",
      " -----> (Rounded results)\n",
      "\n",
      " -----> delta x inputs of layer 0 (tensor prod.):\n",
      "\t[[[-0.    -0.   ]\n",
      "\t  [ 0.     0.   ]\n",
      "\t  [-0.    -0.   ]]\n",
      "\t\n",
      "\t [[ 0.     0.011]\n",
      "\t  [-0.    -0.024]\n",
      "\t  [ 0.     0.02 ]]\n",
      "\t\n",
      "\t [[ 0.01   0.   ]\n",
      "\t  [-0.01  -0.   ]\n",
      "\t  [ 0.015  0.   ]]\n",
      "\t\n",
      "\t [[-0.002 -0.002]\n",
      "\t  [ 0.01   0.01 ]\n",
      "\t  [-0.008 -0.008]]]\n",
      "\n",
      " -----> grad = sum(delta x inputs) of layer 0:\n",
      "\t[[ 0.009  0.009]\n",
      "\t [-0.001 -0.014]\n",
      "\t [ 0.007  0.012]]\n"
     ]
    }
   ],
   "source": [
    "structure=[3,np.shape(labels)[1]]\n",
    "act_func=[\"sigmoid\",\"sigmoid\"]\n",
    "start=[2,]*2\n",
    "\n",
    "netw = verbose_MLP(structure, func=act_func,\n",
    "                   starting_points=start)\n",
    "N_epoch = 1\n",
    "learning_rate=1\n",
    "print(f\"Let's train the model with [epoch: {N_epoch}] and [eta: {learning_rate}]\\n\")\n",
    "netw.train(input_data, labels,\n",
    "           epoch = N_epoch, eta = learning_rate)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
